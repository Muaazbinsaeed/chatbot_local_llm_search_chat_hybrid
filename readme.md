# ğŸ”„ chatbot_local_llm_search_chat_hybrid

A **hybrid AI assistant** that runs **entirely on your machine** and smartâ€‘switches between:

| Mode | What it does |
|------|--------------|
| **LLM only** | Answers from a local Largeâ€‘Languageâ€‘Model served by **Ollama** |
| **Search only** | Performs a realâ€‘time web search (Serper â†’ Google, fallback DuckDuckGo) and returns a summarised answer with sources |
| **Hybrid** | Starts with the LLM, checks if the answer is uncertain, outdated or the user explicitly wants fresh data, then augments the response with a web summary and cited sources |

The UI is built with **Streamlit** and the backend API with **FastAPI**. Everything streams tokenâ€‘byâ€‘token for an ultraâ€‘responsive chat experience.

---

## âœ¨ Key Features

* **Run completely offline** in LLMâ€‘only mode (no OpenAI keys required)
* **Antihallucination guardrails** â€“ the model is instructed to admit when it doesnâ€™t know
* **Web search fallback** with GoogleÂ Serper â†’ DuckDuckGo chain for robustness
* **Automatic answer assessment** â€“ if the LLM response is too short/uncertain the app triggers a web search
* **Live streaming** to both the terminal (API) and the Streamlit frontend
* **Tabbed UX**: Reasoning, Original vs Augmented answer, and list of clickable sources
* **Singleâ€‘file client & server** â€“ easy to read, fork and hack

## ğŸ¥ Live Demo

<div align="center">

<!-- adjust width to taste (e.g. 800, 900â€¦) -->
<img src="assets/p1.gif" width="760"><br>
<strong>Swagger Docs</strong><br><br>

<img src="assets/p2.gif" width="760"><br>
<strong>LLM mode</strong><br><br>

<img src="assets/p3.gif" width="760"><br>
<strong>Internet Search</strong><br><br>

<img src="assets/p4.gif" width="760"><br>
<strong>Hybrid (LLM + Search)</strong>

</div>




---

## ğŸ–¼ï¸ Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚    FastAPI     â”‚
â”‚     UI        â”‚   /chat/*, /searchâ”‚    server.py   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”¤
          â–²                         â”‚  Ollama LLM  â”‚ â”‚
          â”‚ WebSockets/Streaming    â”‚  Google/     â”‚ â”‚
          â”‚                         â”‚  DDG search  â”‚ â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Summariser  â”‚ â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”˜
```

1. **client.py** sends your prompt to `/chat/llm_only`, `/search`, or `/chat/hybrid`.
2. **server.py** streams back tokens. In hybrid mode it decides whether to invokeÂ `get_web_results()` + T5 summariser.
3. Streamlit formats the incoming text into tabs and colourâ€‘coded callouts.

---

## ğŸš€ QuickÂ Start

> Tested on macOSÂ &Â Linux with **PythonÂ 3.11** and **conda**.

```bash
# 1. Clone
$ git clone https://github.com/yourâ€‘username/chatbot_local_llm_search_chat_hybrid.git
$ cd chatbot_local_llm_search_chat_hybrid

# 2. Create environment
$ conda create -n ai-chatbot2 python=3.11 -y
$ conda activate ai-chatbot2
$ pip install -r requirements.txt

# 3. Start the local LLM (400â€¯MB TinyLlama)
$ ollama pull tinyllama:chat
$ ollama serve &

# 4. Export your Serper key (needed for web search)
$ export SERPER_API_KEY="<YOUR_SERPER_KEY>"

# 5. Run backend + frontend
$ uvicorn server:app --reload
$ streamlit run client.py
```
Then open the Streamlit URL â†’ enjoy chatting!

---

## ğŸ”§ Configuration

| Variable | Default | Purpose |
|----------|---------|---------|
| `SERPER_API_KEY` | *required* | GoogleÂ Serper API key for web search |
| `OLLAMA_MODEL`   | `tinyllama:chat` | Change to `phi3`, `llama3`, etc. |

Modify `server.py` to tune:
* **`needs_web_search()`** heuristics
* summariser model in `pipeline()`

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€ client.py      # Streamlit frontend
â”œâ”€ server.py      # FastAPI backend & AI logic
â”œâ”€ requirements.txt
â””â”€ README.md
```

---

## ğŸ› ï¸ Development Tips

* Use **hotâ€‘reload** with `--reload` (FastAPI) and Streamlitâ€™s "alwaysâ€‘rerun".
* The backend exposes three endpoints â€“ curl them directly for CLI testing.
* Replace TinyLlama with any Ollamaâ€‘compatible model (e.g. `llama3`, `mistral`).
* To deploy, containerise with Docker and expose the two processes via `dockerâ€‘compose` or a single image usingÂ `supervisord`.

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch (`git checkout -b feat/awesome`)
3. Commit your changes (`git commit -m 'Add awesome feature'`)
4. Push to GitHub (`git push origin feat/awesome`)
5. Open a PR ğŸ™Œ

---

## ğŸ“œ License

MIT â€“ free for personal & commercial use. Just keep the copyright notice.

---

## ğŸ™ Push to GitHub

If you havenâ€™t pushed yet:
```bash
# initialise repo if needed
git init
git add .
git commit -m "Initial commit â€“ hybrid local LLM chatbot"
# change the URL below
git remote add origin git@github.com:muaazbinsaeed/chatbot_local_llm_search_chat_hybrid.git
git push -u origin main
```

### ğŸ“¸ Sample Outputs

**Prompt â†’ â€œTell me about *myself*â€**

| LLM | Search | Hybrid |
|-----|--------|--------|
| ![LLM-about-myself](assets/pic_LLM%20about%20myself.png) | ![Search-about-myself](assets/pic_search%20about%20myself.png) | ![Hybrid-about-myself](assets/pic_LLM%26Search%20about%20myself.png) |

---

**Prompt â†’ â€œTell me about *Quantum Computing*â€**

| LLM | Search | Hybrid |
|-----|--------|--------|
| ![LLM-about-topic](assets/pic_LLM%20about%20topic.png) | ![Search-about-topic](assets/pic_search%20about%20topic.png) | ![Hybrid-about-topic](assets/pic_LLM%26search%20about%20topic.png) |


Happy hacking! ğŸ‰

